{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wenzhixin (文翼)',\n",
       " 'angus-c (angus croll)',\n",
       " 'yujincheng08 (LoveSy)',\n",
       " 'bokuweb (bokuweb)',\n",
       " 'oerdnj (Ondřej Surý)',\n",
       " 'hanxiao (Han Xiao)',\n",
       " 'JeffreySu (Jeffrey Su)',\n",
       " 'tanersener (Taner Şener)',\n",
       " 'dodyg (Dody Gunawinata)',\n",
       " 'rusty1s (Matthias Fey)',\n",
       " 'mdlayher (Matt Layher)',\n",
       " 'jimmywarting (Jimmy Wärting)',\n",
       " 'dapplion (Lion - dapplion)',\n",
       " 'samuelcolvin (Samuel Colvin)',\n",
       " 'koaning (vincent d warmerdam)',\n",
       " 'pepibumur (Pedro Piñera Buendía)',\n",
       " 'jesserockz (Jesse Hills)',\n",
       " 'PuruVJ (Puru Vijay)',\n",
       " 'dtolnay (David Tolnay)',\n",
       " 'drwpow (Drew Powers)',\n",
       " 'JohnSundell (John Sundell)',\n",
       " 'mrdoob (Mr.doob)',\n",
       " 'rmosolgo (Robert Mosolgo)',\n",
       " 'keegancsmith (Keegan Carruthers-Smith)',\n",
       " 'mikecao (Mike Cao)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "tags          = soup.find_all(\"h1\", {\"class\": \"h3 lh-condensed\"})\n",
    "name_largo    = [tag.getText().strip() for tag in tags]\n",
    "\n",
    "tags          = soup.find_all(\"p\", {\"class\": \"f4 text-normal mb-1\"})\n",
    "name_corto    = [tag.getText().strip() for tag in tags]\n",
    "\n",
    "lista_nombres =[]\n",
    "for nombre1 , nombre2 in zip(name_corto , name_largo) :\n",
    "    lista_nombres.append( nombre1 + \" (\" + nombre2 + \")\")\n",
    "lista_nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-mini-projects',\n",
       " 'datasets',\n",
       " 'espnet',\n",
       " 'devops-exercises',\n",
       " 'pytest',\n",
       " 'fastapi',\n",
       " 'opennsfw2',\n",
       " 'AnimeGANv2',\n",
       " 'CVE-2021-40539',\n",
       " 'transformers',\n",
       " 'netbox',\n",
       " 'public-apis',\n",
       " 'deep-learning-for-image-processing',\n",
       " 'ansible',\n",
       " 'PayloadsAllTheThings',\n",
       " 'fairseq',\n",
       " 'content',\n",
       " 'PyGithub',\n",
       " 'rembg',\n",
       " 'models',\n",
       " 'mlflow',\n",
       " 'mmdetection',\n",
       " 'frigate',\n",
       " 'cupy',\n",
       " 'Python-100-Days']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "tags          = soup.find_all(\"h1\", {\"class\": \"h3 lh-condensed\"})\n",
    "repos         = [tag.getText().strip() for tag in tags ]\n",
    "repos_list    = [ i.split()[2] for i in repos]\n",
    "repos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "images          = soup.find_all(\"a\", {\"class\": \"image\"})\n",
    "images_list     = [\"https:\" + x.get(\"src\") for y in images for x in y ]\n",
    "images_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1048703433',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "links             = soup.find_all(\"a\", href = True)\n",
    "links_list        = [link.get('href') for link in links]\n",
    "links_list_limpio = [link for link in links_list if \"https\" in link]\n",
    "links_list_limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "titles    = soup.find_all(\"div\", {\"class\": \"usctitlechanged\"})\n",
    "titles    = [ title.getText().strip() for title in titles]\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:25:51</td>\n",
       "      <td>8.30</td>\n",
       "      <td>120.64</td>\n",
       "      <td>FLORES REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:21:08</td>\n",
       "      <td>37.05</td>\n",
       "      <td>37.25</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:15:23</td>\n",
       "      <td>18.38</td>\n",
       "      <td>74.28</td>\n",
       "      <td>HAITI REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:14:47</td>\n",
       "      <td>43.43</td>\n",
       "      <td>172.13</td>\n",
       "      <td>SOUTH ISLAND OF NEW ZEALAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:12:47</td>\n",
       "      <td>38.25</td>\n",
       "      <td>38.83</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:08:05</td>\n",
       "      <td>38.28</td>\n",
       "      <td>38.80</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>10:05:18</td>\n",
       "      <td>44.49</td>\n",
       "      <td>8.55</td>\n",
       "      <td>NORTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>09:52:30</td>\n",
       "      <td>13.09</td>\n",
       "      <td>87.81</td>\n",
       "      <td>GOLFO DE FONSECA, EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>09:51:03</td>\n",
       "      <td>39.89</td>\n",
       "      <td>28.74</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>09:48:01</td>\n",
       "      <td>38.27</td>\n",
       "      <td>38.83</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time latitude longitude                         region\n",
       "0  2021-11-12  10:25:51     8.30    120.64       FLORES REGION, INDONESIA\n",
       "1  2021-11-12  10:21:08    37.05     37.25                 CENTRAL TURKEY\n",
       "2  2021-11-12  10:15:23    18.38     74.28                   HAITI REGION\n",
       "3  2021-11-12  10:14:47    43.43    172.13    SOUTH ISLAND OF NEW ZEALAND\n",
       "4  2021-11-12  10:12:47    38.25     38.83                 EASTERN TURKEY\n",
       "5  2021-11-12  10:08:05    38.28     38.80                 EASTERN TURKEY\n",
       "6  2021-11-12  10:05:18    44.49      8.55                 NORTHERN ITALY\n",
       "7  2021-11-12  09:52:30    13.09     87.81  GOLFO DE FONSECA, EL SALVADOR\n",
       "8  2021-11-12  09:51:03    39.89     28.74                 WESTERN TURKEY\n",
       "9  2021-11-12  09:48:01    38.27     38.83                 EASTERN TURKEY"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "coordinates   = [c.getText().strip() for c in soup.find_all(\"td\", {\"class\":\"tabev1\"})]\n",
    "lat=[]\n",
    "lon=[]\n",
    "for i in range(len(coordinates)):\n",
    "    if i%2 == 0:\n",
    "        lat.append(coordinates[i])\n",
    "    else:\n",
    "        lon.append(coordinates[i])\n",
    "\n",
    "\n",
    "earthquakes_list = pd.DataFrame(columns = ['date', 'time', 'latitude', 'longitude', 'region'])\n",
    "earthquakes_list['date']      = [c.getText().strip()[10:20] for c in soup.find_all(\"td\", {\"class\":\"tabev6\"})]\n",
    "earthquakes_list['time']      = [c.getText().strip()[23:31] for c in soup.find_all(\"td\", {\"class\":\"tabev6\"})]\n",
    "earthquakes_list['latitude']  = lat\n",
    "earthquakes_list['longitude'] = lon\n",
    "earthquakes_list['region']    = [c.getText().strip() for c in soup.find_all(\"td\", {\"class\":\"tb_region\"})]\n",
    "\n",
    "earthquakes_list.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '6383000+'),\n",
       " ('日本語', '1292000+'),\n",
       " ('Русский', '1756000+'),\n",
       " ('Deutsch', '2617000+'),\n",
       " ('Español', '1717000+'),\n",
       " ('Français', '2362000+'),\n",
       " ('中文', '1231000+'),\n",
       " ('Italiano', '1718000+'),\n",
       " ('Português', '1074000+'),\n",
       " ('Polski', '1490000+')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "languages = [c.getText().strip().split()[0] for c in soup.find_all(\"a\", {\"class\":\"link-box\"}) ]\n",
    "articles  = [''.join(c.getText().strip().split()[1:4]) for c in soup.find_all(\"a\", {\"class\":\"link-box\"}) ]\n",
    "list(zip(languages , articles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transfer Pricing Management Information System',\n",
       " 'Correspondence Case Record – Treat Official',\n",
       " 'Individual Savings Accounts Main Tables 9.4',\n",
       " 'Large Business Service Hit Rate',\n",
       " 'Individual Savings Accounts Main Tables 9.6',\n",
       " 'Corporate Venture Schemes',\n",
       " 'UK House Price Index',\n",
       " 'VAT Information Exchange System',\n",
       " 'International Trade',\n",
       " 'Employer Compliance System (ECS)',\n",
       " 'VAT Information Exchange System (VIES)',\n",
       " 'HMRC Data Catalogue',\n",
       " 'Monthly Overseas Trade Statistics datasets and importers details',\n",
       " 'Tobacco Duties Statistical Bulletin',\n",
       " 'Single Business Identifier',\n",
       " 'Environmental Tax Bulletin',\n",
       " 'Value Added Tax (VAT) - live mainframe data',\n",
       " 'Corporation Tax Liabilities',\n",
       " 'Large Business Strategy (LBS) Core System - Risk management system',\n",
       " 'UK Regional Trade in Goods estimates']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/search?filters%5Btopic%5D=Business+and+economy'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "datasets = [c.getText() for c in soup.find_all(\"a\",{\"class\": \"govuk-link\"})][5:]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>native_speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Punjabi</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>German</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      language native_speakers\n",
       "0     Mandarin             955\n",
       "1      Spanish             405\n",
       "2      English             360\n",
       "3        Hindi             310\n",
       "4       Arabic             295\n",
       "5   Portuguese             215\n",
       "6      Bengali             205\n",
       "7      Russian             155\n",
       "8     Japanese             125\n",
       "9      Punjabi             100\n",
       "10      German              95"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "tablas    = soup.find_all(\"table\")\n",
    "\n",
    "tabla = tablas[3]\n",
    "\n",
    "languages = []\n",
    "for tr in tabla.find_all(\"tr\") :\n",
    "    fila = [td for td in tr.find_all(\"td\")]\n",
    "    if len(fila) >1 and len(languages) <= 10 :\n",
    "        language_nuevo = {\"language\"        : fila[1].find(\"a\").getText(),\n",
    "                          \"native_speakers\" : fila[2].getText().split(\"(\")[1].replace(\")\",\"\")\n",
    "                         }\n",
    "        languages.append(language_nuevo)\n",
    "languages = pd.DataFrame(languages)\n",
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>initial_release</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>[Tim Robbins, Morgan Freeman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[Marlon Brando, Al Pacino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>[Al Pacino, Robert De Niro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>[Christian Bale, Heath Ledger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>[Henry Fonda, Lee J. Cobb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>246</td>\n",
       "      <td>Las noches de Cabiria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>[Giulietta Masina, François Périer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>247</td>\n",
       "      <td>La princesa prometida</td>\n",
       "      <td>1987</td>\n",
       "      <td>Rob Reiner</td>\n",
       "      <td>[Cary Elwes, Mandy Patinkin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248</td>\n",
       "      <td>Paris, Texas</td>\n",
       "      <td>1984</td>\n",
       "      <td>Wim Wenders</td>\n",
       "      <td>[Harry Dean Stanton, Nastassja Kinski]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>249</td>\n",
       "      <td>Tres colores: Rojo</td>\n",
       "      <td>1994</td>\n",
       "      <td>Krzysztof Kieslowski</td>\n",
       "      <td>[Irène Jacob, Jean-Louis Trintignant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250</td>\n",
       "      <td>Mandarinas</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>[Lembit Ulfsak, Elmo Nüganen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ranking             movie_name initial_release              director  \\\n",
       "0         1        Cadena perpetua            1994        Frank Darabont   \n",
       "1         2             El padrino            1972  Francis Ford Coppola   \n",
       "2         3   El padrino: Parte II            1974  Francis Ford Coppola   \n",
       "3         4    El caballero oscuro            2008     Christopher Nolan   \n",
       "4         5  12 hombres sin piedad            1957          Sidney Lumet   \n",
       "..      ...                    ...             ...                   ...   \n",
       "245     246  Las noches de Cabiria            1957      Federico Fellini   \n",
       "246     247  La princesa prometida            1987            Rob Reiner   \n",
       "247     248           Paris, Texas            1984           Wim Wenders   \n",
       "248     249     Tres colores: Rojo            1994  Krzysztof Kieslowski   \n",
       "249     250             Mandarinas            2013        Zaza Urushadze   \n",
       "\n",
       "                                      stars  \n",
       "0             [Tim Robbins, Morgan Freeman]  \n",
       "1                [Marlon Brando, Al Pacino]  \n",
       "2               [Al Pacino, Robert De Niro]  \n",
       "3            [Christian Bale, Heath Ledger]  \n",
       "4                [Henry Fonda, Lee J. Cobb]  \n",
       "..                                      ...  \n",
       "245     [Giulietta Masina, François Périer]  \n",
       "246            [Cary Elwes, Mandy Patinkin]  \n",
       "247  [Harry Dean Stanton, Nastassja Kinski]  \n",
       "248   [Irène Jacob, Jean-Louis Trintignant]  \n",
       "249           [Lembit Ulfsak, Elmo Nüganen]  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "movies   = [c.getText().strip().split(\"\\n\") for c in soup.find_all(\"td\",{'class':'titleColumn'})]\n",
    "\n",
    "movies_list = pd.DataFrame(columns = ['ranking', 'movie_name', 'initial_release', 'director', 'stars'])\n",
    "movies_list['ranking']         = [c[0].replace(\".\",\"\") for c in movies ]\n",
    "movies_list['movie_name']      = [c[1].strip() for c in movies ]\n",
    "movies_list['initial_release'] = [c[2].replace(\"(\",\"\").replace(\")\",\"\") for c in movies ]\n",
    "\n",
    "titles = [ y.get('title').split(\"(dir.)\") for x in [ i.find_all(\"a\")  \n",
    "                                                    for i in soup.find_all(\"td\",{'class':'titleColumn'})] for y in x]\n",
    "movies_list['director']        = [c[0].strip() for c in titles]\n",
    "movies_list['stars']           = [c[1][2:].split(\", \") for c in titles]\n",
    "\n",
    "movies_list\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
